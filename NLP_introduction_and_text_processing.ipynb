{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9242cq_tpRI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q.1 What is computational linguistics and how does it relate to NLP?"
      ],
      "metadata": {
        "id": "WGsVO-QduG_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computational linguistics is the scientific study of language using computational methods. it focuses on how computers can be programmed to process, understand, and generate human language.\n",
        "\n",
        "## In simple terms, it combines linguistics and computer science to make machines handle language intelligently.\n",
        "\n",
        "## 1. Relationship to NLP\n",
        " - NLP is the appilcation side of    computational linguistics.\n",
        " -While computational linguistics focuses on understandind how language works through computational theories.\n",
        "\n",
        "NLP uses these theories to build real-world applications like:\n",
        "\n",
        "  - Chatbots (e.g., ChatGPT)\n",
        "  - Machine Translation (e.g., Google Translate)\n",
        "  - Sentiment Analysis\n",
        "  - Speech Recognition\n",
        "  - Text Summarization\n"
      ],
      "metadata": {
        "id": "vYyr5-THvO7V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIh_9ioIuD7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lS0DSh0AwdXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Briefly describe the historical evolution of natural language processing."
      ],
      "metadata": {
        "id": "8vTqK1JSweZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## History of NLP\n",
        "1. Turing test(1950):\n",
        "- Alan turing's 'turing test' had an objective to distinguish if machine can exhibit intelligent behavior.\n",
        "\n",
        "\n",
        "2. George IBM experiment:\n",
        "- The objective was to automatically translate 60 russian sentence to english. the feasibility of machine translation was achieved, driving a subsequent and sparking interest in NLP.\n",
        "\n",
        "\n",
        "3. Eliza (1964-1966):\n",
        "- Use pattern matching to simulate conversations.\n",
        "- They were able to simulate conversations.\n",
        "- They were able to create the first chatbot mimicking a psychotherapist, demonstrating basic text interaction.\n",
        "\n",
        "\n",
        "4. Lunar system (1971):\n",
        "- The objective was to answer questions about mnoon rocks usig natural language.\n",
        "- This is one if the  earliest e.g of\n",
        "question answering system.\n",
        "\n",
        "\n",
        "5. POS tagging (1980):\n",
        "- The objective was to assign grammatical categories to words.\n",
        "- improved parsing abd understanding of sentence structure was achieved.\n",
        "\n",
        "\n",
        "6. Statistical machine translation(1986):\n",
        " - The objective was to translate text based on statistical pattern.\n",
        " - The shift from rule based method to data driven approaches in machine translation.\n",
        "\n",
        "\n",
        "7. 1990 (Wordnet):\n",
        "- The objective was to creast a lexical database in english language.\n",
        "\n",
        "\n",
        "8. Bert (2018):\n",
        "- Enable bidirectional understanding of language context/\n",
        "- Improved performance across various NLP  benchmarks,including question answering.\n"
      ],
      "metadata": {
        "id": "tJqBn40twueG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHr5FNlz0uw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CL-GKJYw1GKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. List and explain three major use cases of NLP in today's tech industry."
      ],
      "metadata": {
        "id": "UVUIauqe1GqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Chatbots and virtual assitants\n",
        "   - Example: Chatgpt,Siri,Alexa,  Google assistant\n",
        "   - Explanation:\n",
        "   NLP allow  these systems to understand user questions and generate human like responses.\n",
        "\n",
        "\n",
        "2. Sentiment analysis\n",
        "   - Example: Used by companies like Amazon, Twitter,and Youtube\n",
        "   - Explanation: NLP analyzes text date to determine whether the sentiment is positive,negative, or neutral.\n",
        "\n",
        "3. Machine learning\n",
        "   - Example: Google translate, deeml, Microsoft translator\n",
        "   - Explanation: NLP-powered tranlastion systems converts text or speech  from one language to another by understanding context, grammar, and meaning.\n",
        "\n"
      ],
      "metadata": {
        "id": "YcTX8HFi1siZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XDzicSqm1rg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JMaWRxfo3kOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4.  What is text normalization and why is it essential in text processing tasks?"
      ],
      "metadata": {
        "id": "nLnfGpUZ3kr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text normalization\n",
        "\n",
        "Text normalization is the process of converting text into  a standard, consistent, and uniform format sthat it can be  easily proxessed by NLP models or algorithms.\n",
        "\n",
        "  - Example:\n",
        "    -  Raw text: \" I LOOOOVE NLP!!! IT IS AMAZIN\"\n",
        "    -  After normalization : \"i love nlp it is amazing\"\n",
        "\n",
        "ğŸ”¹Common steps in text normalization\n",
        "   1. Lowercasing: converts all text to lowercase\n",
        "      - Example: \"ChatGPT\" --> \"chatgpt\"\n",
        "\n",
        "   2. Removing punctuation: Deletes symbols and special characters\n",
        "       - Example: \"hello!!!\" --> \"hello\"\n",
        "\n",
        "   3. Tokenization: Splits text into individual words or tokens\n",
        "       - Example: \"I love AI\"--> [\"I\",\"love\",'AI']\n",
        "\n",
        "\n",
        "   4. Lemmatization/Stemming: Converts words to their base form\n",
        "       -  Example: \"running\",\"ran\"--> \"run\"\n",
        "\n",
        "   5. Removing stopwords: Removes common words with little meaning.\n",
        "        - Example: \"is\",\"a\",\"the\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Zr860WYO4Koi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dUI6Atzb4DIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U9L3kCD1_UiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Compare and contrast stemming and lemmatization with suitable examples."
      ],
      "metadata": {
        "id": "9g97gAMsAnAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature         | Stemming                             | Lemmatization                   |\n",
        "| --------------- | ------------------------------------ | ------------------------------- |\n",
        "| Approach        | Applies simple rules to chop endings | Uses vocabulary + grammar rules |\n",
        "| Output          | May not be a real word               | Always a valid dictionary word  |\n",
        "| Accuracy        | Less accurate                        | Highly accurate                 |\n",
        "| Speed           | Faster                               | Slower                          |\n",
        "| Handles Context | âŒ No                                 | âœ” Yes                           |\n",
        "| Example Tools   | Porter Stemmer, Snowball Stemmer     | WordNet Lemmatizer              |\n"
      ],
      "metadata": {
        "id": "XSwoPVDWBAy4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZMgDzQyAnh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udFW5zGHAn8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. Write a python program that uses regular expressions(regex) to extract all email addresses from the following block of text."
      ],
      "metadata": {
        "id": "2G6zA4oK_oDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â€œHello team, please contact us at support@xyz.com for technical issues, or reach out to our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.â€"
      ],
      "metadata": {
        "id": "8teIytb0ASTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org\n",
        "and jenny via jenny_clarke126@mail.co.us. For partnership inquiries,\n",
        "email partners@xyz.biz.\"\"\"\n",
        "\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "print(\"Extracted Email Addresses:\")\n",
        "for email in emails:\n",
        "    print(email)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvmqf3UcAQUf",
        "outputId": "5c124595-639f-403d-8aac-d2cd0b3efc6c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4uCgzEEAhg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tb85CR-BDm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:"
      ],
      "metadata": {
        "id": "gQDS4oMSBECE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â€œNatural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.â€"
      ],
      "metadata": {
        "id": "3SlbfZGtBOPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand, interpret,\n",
        "and generate human language. Applications of NLP include chatbots, sentiment analysis,\n",
        "and machine translation. As technology advances, the role of NLP in modern solutions is\n",
        "becoming increasingly critical.\"\"\"\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "freq = FreqDist(tokens)\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, count in freq.items():\n",
        "    print(word, \":\", count)\n",
        "\n",
        "\n",
        "print(\"\\nMost Common Words:\")\n",
        "print(freq.most_common(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "14ikI-42BJH4",
        "outputId": "c8d1bc53-beae-4c39-a1e3-b3c2a1d8a078"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3661509287.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokens:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYCqm1uCBta4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__TLCaGqBiBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text."
      ],
      "metadata": {
        "id": "axYJbMdxBt5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "text = \"\"\"John lives in New York and works at Microsoft.\n",
        "He recently met Sarah in London.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Proper Noun Annotations:\\n\")\n",
        "for token in doc:\n",
        "    if token.pos_ == \"PROPN\": \" I'm sorry, I'm having trouble answering that question right now.\"\n",
        "        print(f\"{token.text} --> PROPER NOUN\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61Ljo02HB9IS",
        "outputId": "9c9c3679-b1ff-4dbc-af04-7d859db4b83c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proper Noun Annotations:\n",
            "\n",
            "John --> PROPER NOUN\n",
            "New --> PROPER NOUN\n",
            "York --> PROPER NOUN\n",
            "Microsoft --> PROPER NOUN\n",
            "Sarah --> PROPER NOUN\n",
            "London --> PROPER NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LicAkjrxCPaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AgsZE3OECP54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q9. Using Genism, demonstrate how to train a simple Word2Vec model on the following dataset consisting of example sentences:\n",
        "\n",
        "dataset = [ \"Natural language processing enables computers to understand human language\", \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\", \"Word2Vec is a popular word embedding technique used in many NLP applications\", \"Text preprocessing is a critical step before training word embeddings\", \"Tokenization and normalization help clean raw text for modeling\" ] Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using Gensim. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "FIONbX9tB0Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download tokenizer (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Preprocessing Function\n",
        "# -------------------------------------------------------------\n",
        "def preprocess(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing\n",
        "tokenized_data = [preprocess(sentence) for sentence in dataset]\n",
        "\n",
        "print(\"âœ” Tokenized Sentences:\\n\")\n",
        "for sent in tokenized_data:\n",
        "    print(sent)\n",
        "\n",
        "\n",
        "model = Word2Vec(\n",
        "    sentences=tokenized_data,\n",
        "    vector_size=50,       # word vector size\n",
        "    window=5,             # context window\n",
        "    min_count=1,          # include all words\n",
        "    workers=4,            # use all CPU cores\n",
        "    sg=1                  # 1 = Skip-gram, 0 = CBOW\n",
        ")\n",
        "\n",
        "print(\"\\nâœ” Model Training Complete!\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Example Outputs\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\nâœ” Word Vector for 'language':\")\n",
        "print(model.wv[\"language\"])\n",
        "\n",
        "print(\"\\nâœ” Most Similar Words to 'word':\")\n",
        "print(model.wv.most_similar(\"word\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x40rRm8Bziu",
        "outputId": "86c899bf-6743-44b6-b0de-f50c9e7ad417"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ” Tokenized Sentences:\n",
            "\n",
            "['natural', 'language', 'processing', 'enables', 'computers', 'to', 'understand', 'human', 'language']\n",
            "['word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'with', 'similar', 'meaning', 'to', 'have', 'similar', 'representation']\n",
            "['word2vec', 'is', 'a', 'popular', 'word', 'embedding', 'technique', 'used', 'in', 'many', 'nlp', 'applications']\n",
            "['text', 'preprocessing', 'is', 'a', 'critical', 'step', 'before', 'training', 'word', 'embeddings']\n",
            "['tokenization', 'and', 'normalization', 'help', 'clean', 'raw', 'text', 'for', 'modeling']\n",
            "\n",
            "âœ” Model Training Complete!\n",
            "\n",
            "âœ” Word Vector for 'language':\n",
            "[-0.01423318  0.00248706 -0.01436689 -0.00445629  0.00743943  0.01165486\n",
            "  0.00240286  0.00422623 -0.00822258  0.01442754 -0.01262054  0.00929353\n",
            " -0.01645306  0.00407736 -0.00995055 -0.00847234 -0.00620637  0.01129274\n",
            "  0.01159587 -0.00995605  0.0015468  -0.01698489  0.0156155   0.01850834\n",
            " -0.00549848  0.00159803  0.00147035  0.01097319 -0.01718692  0.00117666\n",
            "  0.01371417  0.00444817  0.00226595 -0.01865775  0.01694982 -0.01253469\n",
            " -0.00595705  0.00697844 -0.00156436  0.00282744  0.00356582 -0.01366324\n",
            " -0.0194576   0.01809294  0.01239782 -0.01382946  0.00680742  0.00041243\n",
            "  0.00950931 -0.01421476]\n",
            "\n",
            "âœ” Most Similar Words to 'word':\n",
            "[('before', 0.2705621123313904), ('enables', 0.25514039397239685), ('meaning', 0.240879088640213), ('normalization', 0.2112826704978943), ('nlp', 0.18615314364433289), ('are', 0.1755194514989853), ('raw', 0.1674014925956726), ('applications', 0.16179609298706055), ('help', 0.15040813386440277), ('popular', 0.14584149420261383)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_45Ro6I9C5gX",
        "outputId": "e5f1fbe2-c1c5-46c2-ad40-01122514d51e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rQYLRz2Dikm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-QbsYyUEDjAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q10.  Imagine you are a data scientist at a fintech startup. Youâ€™ve been tasked with analyzing customer feedback. Outline the steps you would take to clean, process, and extract useful insights using NLP techniques from thousands of customer review"
      ],
      "metadata": {
        "id": "MdnpmNpcDkDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a **more natural, human-like, story-style** version of the answer â€” as if a real data scientist is explaining their thought process.\n",
        "\n",
        "---\n",
        "\n",
        "# â­ **How I Would Analyze Thousands of Customer Reviews as a Data Scientist at a Fintech Startup**\n",
        "\n",
        "If I were handling thousands of customer reviews at a fintech company, hereâ€™s exactly how I would approach it â€” step by step, in a practical and human way.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Start by Understanding What the Company Wants**\n",
        "\n",
        "Before doing anything technical, Iâ€™d sit down with the product and support teams to understand:\n",
        "\n",
        "* What problems are customers facing most?\n",
        "* Are transactions failing? Is the app slow? Are people unhappy with loan approvals?\n",
        "* Do we want to track overall satisfaction or detect specific issues?\n",
        "\n",
        "This gives clear direction for the analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Collect All the Feedback in One Place**\n",
        "\n",
        "Customer reviews are scattered everywhere â€” Play Store, App Store, social media, emails, support tickets.\n",
        "My first job is to bring all these into a single dataset so we can see the full picture.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Clean the Messy Text**\n",
        "\n",
        "Real-world text is messy, so Iâ€™d:\n",
        "\n",
        "* Remove emojis, special characters, links\n",
        "* Convert everything to lowercase\n",
        "* Remove duplicate reviews\n",
        "* Handle missing or broken text\n",
        "\n",
        "The goal is to make the text readable and consistent.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Preprocess the Text to Make It NLP-Friendly**\n",
        "\n",
        "Before analysis, the text must be broken down properly:\n",
        "\n",
        "* **Tokenize** it (break sentences into words)\n",
        "* **Remove stopwords** (â€œtheâ€, â€œisâ€, â€œatâ€)\n",
        "* **Lemmatize** words so that â€œdelayedâ€, â€œdelaysâ€, â€œdelayâ€ all become **â€œdelayâ€**\n",
        "* Optionally fix spelling mistakes, expand short forms (â€œcanâ€™tâ€ â†’ â€œcannotâ€)\n",
        "\n",
        "This step helps reduce noise and improves NLP model accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Explore What Customers Are Talking About**\n",
        "\n",
        "Here, Iâ€™m trying to get a sense of the bigger picture:\n",
        "\n",
        "* Which words appear most often?\n",
        "* What 2â€“3-word phrases are common (e.g., â€œpayment failedâ€, â€œloan issueâ€, â€œUPI errorâ€)?\n",
        "* What topics come up again and again?\n",
        "\n",
        "This helps me understand customers even before building any models.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Run Sentiment Analysis**\n",
        "\n",
        "Now I want to know how customers **feel**:\n",
        "\n",
        "* Are reviews mostly positive, neutral, or negative?\n",
        "* Is sentiment getting worse after a certain app update?\n",
        "* Are some features making people frustrated?\n",
        "\n",
        "I might use VADER for quick analysis or BERT for deeper accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Identify Key Themes Using Topic Modeling**\n",
        "\n",
        "With thousands of reviews, the best way to find hidden patterns is to use topic modeling.\n",
        "This automatically tells me:\n",
        "\n",
        "* Are loans getting delayed?\n",
        "* Are KYC issues rising?\n",
        "* Are customers frustrated with customer support?\n",
        "* Is the app crashing frequently?\n",
        "\n",
        "These insights help product teams prioritize what to fix first.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Extract Important Information (NER)**\n",
        "\n",
        "Using NER, Iâ€™d pull out:\n",
        "\n",
        "* Product names\n",
        "* Banks\n",
        "* Error types\n",
        "* Transaction IDs\n",
        "* Locations\n",
        "\n",
        "This helps identify issues tied to specific banks or features.\n",
        "\n"
      ],
      "metadata": {
        "id": "XMM31hJVD_tj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lrzWT90lC-Li"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}